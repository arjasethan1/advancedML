{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load data\n",
    "item_categories = pd.read_csv('data/item_categories.csv')\n",
    "items = pd.read_csv('data/items.csv')\n",
    "shops = pd.read_csv('data/shops.csv')\n",
    "sales_train = pd.read_csv('data/sales_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214200, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111404, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_train= pd.DataFrame(columns=['shop_id', 'item_id', 'date_block_num', 'item_price']) #'max_price', 'min_price', 'avg_price'])\n",
    "for block_num in np.sort(sales_train.date_block_num.unique())[::-1]:\n",
    "    test_dummy = test.merge(sales_train[sales_train.date_block_num == block_num], on=['shop_id', 'item_id'])\n",
    "    test_dummy = test_dummy.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False).agg({'item_price' : 'mean'})#{'max_price':'max', 'min_price':'min', 'avg_price': 'mean'}})\n",
    "#     test_dummy.columns = [col[0] if col[-1]=='' else col[-1] for col in test_dummy.columns.values]\n",
    "#     test_dummy.head(10)\n",
    "    test_train =pd.concat([test_train,test_dummy], axis=0)\n",
    "test_train = test_train.merge(test_train.groupby(['shop_id', 'item_id'], as_index= False).agg({'date_block_num': np.max}), on=['shop_id', 'item_id', 'date_block_num'])\n",
    "\n",
    "del test_dummy\n",
    "gc.collect();\n",
    "test_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>item_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5037</td>\n",
       "      <td>34</td>\n",
       "      <td>749.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5320</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5233</td>\n",
       "      <td>34</td>\n",
       "      <td>1199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5232</td>\n",
       "      <td>34</td>\n",
       "      <td>599.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5268</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  shop_id  item_id  date_block_num  item_price\n",
       "0   0        5     5037              34       749.5\n",
       "1   1        5     5320              34         0.0\n",
       "2   2        5     5233              34      1199.0\n",
       "3   3        5     5232              34       599.0\n",
       "4   4        5     5268              34         0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_train = pd.merge(test, test_train, how='left', on=['shop_id', 'item_id'])\n",
    "test_train.date_block_num = 34\n",
    "test_train = test_train.fillna(0)\n",
    "test_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/pandas/core/groupby.py:3961: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales_train['date_block_num'].unique():\n",
    "    cur_shops = sales_train[sales_train['date_block_num']==block_num]['shop_id'].unique()\n",
    "    cur_items = sales_train[sales_train['date_block_num']==block_num]['item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "#turn the grid into pandas dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "#get aggregated values for (shop_id, item_id, month)\n",
    "gb = sales_train.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}}) #, 'item_price' : {'max_price':'max', 'min_price':'min', 'avg_price': 'mean'}})\n",
    "\n",
    "#fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "#join aggregated data to the grid\n",
    "all_data = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n",
    "#sort the data\n",
    "all_data.sort_values(['date_block_num','shop_id','item_id'],inplace=True)\n",
    "all_data = pd.concat([all_data, test_train.drop('ID', axis=1)], axis=0)\n",
    "\n",
    "# # Groupby data to get shop-item-month aggregates\n",
    "# gb = sales_train.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# # Fix column names\n",
    "# gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# # Join it to the grid\n",
    "# all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales_train.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales_train.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "## create future using item category\n",
    "all_data = all_data.merge(items[['item_id','item_category_id']], how='left', on = ['item_id'])\n",
    "sales_train_dummy = sales_train.merge(items, how='left', on=['item_id'])\n",
    "gb = sales_train_dummy.groupby(['item_category_id', 'date_block_num'], as_index=False).agg({'item_cnt_day': {'target_item_category' : 'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_category_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb, sales_train_dummy\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>target_item_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10913850</th>\n",
       "      <td>34</td>\n",
       "      <td>5037</td>\n",
       "      <td>749.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913851</th>\n",
       "      <td>34</td>\n",
       "      <td>5320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913852</th>\n",
       "      <td>34</td>\n",
       "      <td>5233</td>\n",
       "      <td>1199.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913853</th>\n",
       "      <td>34</td>\n",
       "      <td>5232</td>\n",
       "      <td>599.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913854</th>\n",
       "      <td>34</td>\n",
       "      <td>5268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913855</th>\n",
       "      <td>34</td>\n",
       "      <td>5039</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913856</th>\n",
       "      <td>34</td>\n",
       "      <td>5041</td>\n",
       "      <td>3999.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913857</th>\n",
       "      <td>34</td>\n",
       "      <td>5046</td>\n",
       "      <td>349.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913858</th>\n",
       "      <td>34</td>\n",
       "      <td>5319</td>\n",
       "      <td>299.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913859</th>\n",
       "      <td>34</td>\n",
       "      <td>5003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_block_num  item_id  item_price  shop_id  target  target_shop  \\\n",
       "10913850              34     5037       749.5        5     0.0          0.0   \n",
       "10913851              34     5320         0.0        5     0.0          0.0   \n",
       "10913852              34     5233      1199.0        5     0.0          0.0   \n",
       "10913853              34     5232       599.0        5     0.0          0.0   \n",
       "10913854              34     5268         0.0        5     0.0          0.0   \n",
       "10913855              34     5039      1499.0        5     0.0          0.0   \n",
       "10913856              34     5041      3999.0        5     0.0          0.0   \n",
       "10913857              34     5046       349.0        5     0.0          0.0   \n",
       "10913858              34     5319       299.0        5     0.0          0.0   \n",
       "10913859              34     5003         0.0        5     0.0          0.0   \n",
       "\n",
       "          target_item  item_category_id  target_item_category  \n",
       "10913850          0.0                19                   0.0  \n",
       "10913851          0.0                55                   0.0  \n",
       "10913852          0.0                19                   0.0  \n",
       "10913853          0.0                23                   0.0  \n",
       "10913854          0.0                20                   0.0  \n",
       "10913855          0.0                23                   0.0  \n",
       "10913856          0.0                20                   0.0  \n",
       "10913857          0.0                55                   0.0  \n",
       "10913858          0.0                55                   0.0  \n",
       "10913859          0.0                20                   0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[all_data.date_block_num == 34].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130824b5d1474c9b9e9fae00419b2c9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "index_cols.append('item_category_id')\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "# item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "# all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>target_item_category</th>\n",
       "      <th>item_price_lag_1</th>\n",
       "      <th>...</th>\n",
       "      <th>item_price_lag_5</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_item_category_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>item_price_lag_12</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_item_category_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10913850</th>\n",
       "      <td>34</td>\n",
       "      <td>5037</td>\n",
       "      <td>749.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>3487.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6134.0</td>\n",
       "      <td>1445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913851</th>\n",
       "      <td>34</td>\n",
       "      <td>5320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913852</th>\n",
       "      <td>34</td>\n",
       "      <td>5233</td>\n",
       "      <td>1199.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>3487.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913853</th>\n",
       "      <td>34</td>\n",
       "      <td>5232</td>\n",
       "      <td>599.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913854</th>\n",
       "      <td>34</td>\n",
       "      <td>5268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913855</th>\n",
       "      <td>34</td>\n",
       "      <td>5039</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2984.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>5275.0</td>\n",
       "      <td>1445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913856</th>\n",
       "      <td>34</td>\n",
       "      <td>5041</td>\n",
       "      <td>3999.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913857</th>\n",
       "      <td>34</td>\n",
       "      <td>5046</td>\n",
       "      <td>349.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6017.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9809.0</td>\n",
       "      <td>1445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913858</th>\n",
       "      <td>34</td>\n",
       "      <td>5319</td>\n",
       "      <td>299.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6017.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>9809.0</td>\n",
       "      <td>1445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913859</th>\n",
       "      <td>34</td>\n",
       "      <td>5003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_block_num  item_id  item_price  shop_id  target  target_shop  \\\n",
       "10913850              34     5037       749.5        5     0.0          0.0   \n",
       "10913851              34     5320         0.0        5     0.0          0.0   \n",
       "10913852              34     5233      1199.0        5     0.0          0.0   \n",
       "10913853              34     5232       599.0        5     0.0          0.0   \n",
       "10913854              34     5268         0.0        5     0.0          0.0   \n",
       "10913855              34     5039      1499.0        5     0.0          0.0   \n",
       "10913856              34     5041      3999.0        5     0.0          0.0   \n",
       "10913857              34     5046       349.0        5     0.0          0.0   \n",
       "10913858              34     5319       299.0        5     0.0          0.0   \n",
       "10913859              34     5003         0.0        5     0.0          0.0   \n",
       "\n",
       "          target_item  item_category_id  target_item_category  \\\n",
       "10913850          0.0                19                   0.0   \n",
       "10913851          0.0                55                   0.0   \n",
       "10913852          0.0                19                   0.0   \n",
       "10913853          0.0                23                   0.0   \n",
       "10913854          0.0                20                   0.0   \n",
       "10913855          0.0                23                   0.0   \n",
       "10913856          0.0                20                   0.0   \n",
       "10913857          0.0                55                   0.0   \n",
       "10913858          0.0                55                   0.0   \n",
       "10913859          0.0                20                   0.0   \n",
       "\n",
       "          item_price_lag_1         ...          item_price_lag_5  \\\n",
       "10913850               0.0         ...                       0.0   \n",
       "10913851               0.0         ...                       0.0   \n",
       "10913852               0.0         ...                       0.0   \n",
       "10913853               0.0         ...                       0.0   \n",
       "10913854               0.0         ...                       0.0   \n",
       "10913855               0.0         ...                       0.0   \n",
       "10913856               0.0         ...                       0.0   \n",
       "10913857               0.0         ...                       0.0   \n",
       "10913858               0.0         ...                       0.0   \n",
       "10913859               0.0         ...                       0.0   \n",
       "\n",
       "          target_lag_5  target_item_lag_5  target_item_category_lag_5  \\\n",
       "10913850           1.0              105.0                      3487.0   \n",
       "10913851           0.0                0.0                         0.0   \n",
       "10913852           2.0              119.0                      3487.0   \n",
       "10913853           0.0                0.0                         0.0   \n",
       "10913854           0.0                0.0                         0.0   \n",
       "10913855           0.0               84.0                      2984.0   \n",
       "10913856           0.0                0.0                         0.0   \n",
       "10913857           1.0               21.0                      6017.0   \n",
       "10913858           4.0               56.0                      6017.0   \n",
       "10913859           0.0                0.0                         0.0   \n",
       "\n",
       "          target_shop_lag_5  item_price_lag_12  target_lag_12  \\\n",
       "10913850              954.0                0.0            1.0   \n",
       "10913851                0.0                0.0            0.0   \n",
       "10913852              954.0                0.0            0.0   \n",
       "10913853                0.0                0.0            0.0   \n",
       "10913854                0.0                0.0            0.0   \n",
       "10913855              954.0                0.0            0.0   \n",
       "10913856                0.0                0.0            0.0   \n",
       "10913857              954.0                0.0            1.0   \n",
       "10913858              954.0                0.0            5.0   \n",
       "10913859                0.0                0.0            0.0   \n",
       "\n",
       "          target_item_lag_12  target_item_category_lag_12  target_shop_lag_12  \n",
       "10913850                65.0                       6134.0              1445.0  \n",
       "10913851                 0.0                          0.0                 0.0  \n",
       "10913852                 0.0                          0.0                 0.0  \n",
       "10913853                 0.0                          0.0                 0.0  \n",
       "10913854                 0.0                          0.0                 0.0  \n",
       "10913855                45.0                       5275.0              1445.0  \n",
       "10913856                 0.0                          0.0                 0.0  \n",
       "10913857                29.0                       9809.0              1445.0  \n",
       "10913858               270.0                       9809.0              1445.0  \n",
       "10913859                 0.0                          0.0                 0.0  \n",
       "\n",
       "[10 rows x 39 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[all_data.date_block_num ==34].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 34\n",
      "Val `date_block_num` is 33\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "last_block = dates.max()\n",
    "val_block = dates.max() -1\n",
    "print('Test `date_block_num` is %d' % last_block)\n",
    "print('Val `date_block_num` is %d' % val_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train = dates[dates < val_block]\n",
    "dates_val = dates[dates == val_block]\n",
    "dates_test = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  val_block]\n",
    "X_val = all_data.loc[dates == val_block]\n",
    "X_test =  all_data.loc[dates == last_block]\n",
    "\n",
    "y_train = all_data.loc[dates <  val_block, 'target'].values\n",
    "y_val =  all_data.loc[dates == val_block, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for linreg is 0.470225\n",
      "Train R-squared for linreg is 0.255082\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.drop(to_drop_cols, axis=1).values, y_train)\n",
    "pred_lr = lr.predict(X_val.drop(to_drop_cols, axis=1).values)\n",
    "\n",
    "print('Train R-squared for linreg is %f' % r2_score(y_train, lr.predict(X_train.drop(to_drop_cols, axis=1).values)))\n",
    "print('Train R-squared for linreg is %f' % r2_score(y_val, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_submit(pred_data, path):\n",
    "    submit = pd.merge(test, pred_data, on=['shop_id', 'item_id'], how='left')[[\"ID\", \"item_cnt_month\"]]\n",
    "    submit['item_cnt_month'] = submit.item_cnt_month.fillna(0).clip(0,20)\n",
    "    submit.to_csv(path, sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uncomment for submission(lb score : 1.07770)\n",
    "# test_lr_without_encoding = X_test.copy()\n",
    "# test_lr_without_encoding['item_cnt_month'] = lr.predict(test_lr_without_encoding.drop(to_drop_cols, axis=1).values)\n",
    "# prepare_submit(test_lr_without_encoding, path='data/lr_without_encoding1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#mean encoding\n",
    "\n",
    "# item_id encoding\n",
    "from sklearn.model_selection import KFold\n",
    "target = X_train.target.values\n",
    "kf = KFold(n_splits = 5, shuffle = False)\n",
    "X_train['item_target_enc'] = np.nan\n",
    "for tr_inds, val_inds in kf.split(target):\n",
    "    tr_fold, val_fold = X_train.iloc[tr_inds], X_train.iloc[val_inds]\n",
    "    val_fold['item_target_enc'] = val_fold['item_id'].map(tr_fold.groupby('item_id').target.mean())\n",
    "    X_train.iloc[val_inds, :] = val_fold\n",
    "\n",
    "global_mean = X_train.target.mean()\n",
    "X_train.fillna(global_mean, inplace= True)\n",
    "\n",
    "#shopid_id encoding\n",
    "X_train['shop_target_enc'] = np.nan\n",
    "for tr_inds, val_inds in kf.split(target):\n",
    "    tr_fold, val_fold = X_train.iloc[tr_inds], X_train.iloc[val_inds]\n",
    "    val_fold['shop_target_enc'] = val_fold['shop_id'].map(tr_fold.groupby('shop_id').target.mean())\n",
    "    X_train.iloc[val_inds, :] = val_fold\n",
    "\n",
    "global_mean = X_train.target.mean()\n",
    "X_train.fillna(global_mean, inplace= True)\n",
    "\n",
    "# caategory_id encoding\n",
    "X_train['category_target_enc'] = np.nan\n",
    "for tr_inds, val_inds in kf.split(target):\n",
    "    tr_fold, val_fold = X_train.iloc[tr_inds], X_train.iloc[val_inds]\n",
    "    val_fold['category_target_enc'] = val_fold['item_category_id'].map(tr_fold.groupby('item_category_id').target.mean())\n",
    "    X_train.iloc[val_inds, :] = val_fold\n",
    "\n",
    "global_mean = X_train.target.mean()\n",
    "X_train.fillna(global_mean, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2450: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>target_item_category</th>\n",
       "      <th>item_price_lag_1</th>\n",
       "      <th>...</th>\n",
       "      <th>target_item_category_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>item_price_lag_12</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_item_category_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_target_enc</th>\n",
       "      <th>shop_target_enc</th>\n",
       "      <th>category_target_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4488756</th>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>9282.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10488.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8983.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.585405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488757</th>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>40</td>\n",
       "      <td>22065.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24130.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205596</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.242273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488758</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>37</td>\n",
       "      <td>7511.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8680.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324818</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.159289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488759</th>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>40</td>\n",
       "      <td>22065.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24130.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>33489.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>0.603406</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.242273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488760</th>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>37</td>\n",
       "      <td>7511.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8680.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>6094.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>0.350365</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.159289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488761</th>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>40</td>\n",
       "      <td>22065.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24130.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33489.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.242273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488762</th>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37</td>\n",
       "      <td>7511.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8680.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031311</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.159289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488763</th>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40</td>\n",
       "      <td>22065.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24130.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053459</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.242273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488764</th>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>1257.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.141051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488765</th>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57</td>\n",
       "      <td>984.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1539.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>0.037433</td>\n",
       "      <td>0.161184</td>\n",
       "      <td>0.093589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_block_num  item_id  item_price  shop_id  target  target_shop  \\\n",
       "4488756              12       27         0.0        2     0.0        890.0   \n",
       "4488757              12       30         0.0        2     0.0        890.0   \n",
       "4488758              12       31         0.0        2     0.0        890.0   \n",
       "4488759              12       32         0.0        2     1.0        890.0   \n",
       "4488760              12       33         0.0        2     1.0        890.0   \n",
       "4488761              12       34         0.0        2     0.0        890.0   \n",
       "4488762              12       36         0.0        2     0.0        890.0   \n",
       "4488763              12       37         0.0        2     0.0        890.0   \n",
       "4488764              12       39         0.0        2     0.0        890.0   \n",
       "4488765              12       40         0.0        2     0.0        890.0   \n",
       "\n",
       "         target_item  item_category_id  target_item_category  \\\n",
       "4488756          1.0                19                9282.0   \n",
       "4488757         58.0                40               22065.0   \n",
       "4488758         15.0                37                7511.0   \n",
       "4488759         84.0                40               22065.0   \n",
       "4488760         42.0                37                7511.0   \n",
       "4488761          6.0                40               22065.0   \n",
       "4488762          2.0                37                7511.0   \n",
       "4488763          5.0                40               22065.0   \n",
       "4488764          1.0                41                1257.0   \n",
       "4488765          2.0                57                 984.0   \n",
       "\n",
       "         item_price_lag_1         ...           target_item_category_lag_5  \\\n",
       "4488756               0.0         ...                              10488.0   \n",
       "4488757               0.0         ...                              24130.0   \n",
       "4488758               0.0         ...                               8680.0   \n",
       "4488759               0.0         ...                              24130.0   \n",
       "4488760               0.0         ...                               8680.0   \n",
       "4488761               0.0         ...                              24130.0   \n",
       "4488762               0.0         ...                               8680.0   \n",
       "4488763               0.0         ...                              24130.0   \n",
       "4488764               0.0         ...                                  0.0   \n",
       "4488765               0.0         ...                               1316.0   \n",
       "\n",
       "         target_shop_lag_5  item_price_lag_12  target_lag_12  \\\n",
       "4488756              875.0                0.0            1.0   \n",
       "4488757              875.0                0.0            0.0   \n",
       "4488758              875.0                0.0            0.0   \n",
       "4488759              875.0                0.0            0.0   \n",
       "4488760              875.0                0.0            1.0   \n",
       "4488761              875.0                0.0            0.0   \n",
       "4488762              875.0                0.0            0.0   \n",
       "4488763              875.0                0.0            0.0   \n",
       "4488764                0.0                0.0            0.0   \n",
       "4488765              875.0                0.0            0.0   \n",
       "\n",
       "         target_item_lag_12  target_item_category_lag_12  target_shop_lag_12  \\\n",
       "4488756                 7.0                       8983.0              1146.0   \n",
       "4488757                 0.0                          0.0                 0.0   \n",
       "4488758                 0.0                          0.0                 0.0   \n",
       "4488759               299.0                      33489.0              1146.0   \n",
       "4488760                61.0                       6094.0              1146.0   \n",
       "4488761                 9.0                      33489.0              1146.0   \n",
       "4488762                 0.0                          0.0                 0.0   \n",
       "4488763                 0.0                          0.0                 0.0   \n",
       "4488764                 0.0                          0.0                 0.0   \n",
       "4488765                 4.0                       1539.0              1146.0   \n",
       "\n",
       "         item_target_enc  shop_target_enc  category_target_enc  \n",
       "4488756         0.040816         0.161184             0.585405  \n",
       "4488757         0.205596         0.161184             0.242273  \n",
       "4488758         0.324818         0.161184             0.159289  \n",
       "4488759         0.603406         0.161184             0.242273  \n",
       "4488760         0.350365         0.161184             0.159289  \n",
       "4488761         0.033784         0.161184             0.242273  \n",
       "4488762         0.031311         0.161184             0.159289  \n",
       "4488763         0.053459         0.161184             0.242273  \n",
       "4488764         0.022472         0.161184             0.141051  \n",
       "4488765         0.037433         0.161184             0.093589  \n",
       "\n",
       "[10 rows x 42 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## downcast data type\n",
    "X_train = downcast_dtypes(X_train)\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#mean encoding for test data\n",
    "X_val['item_target_enc'] = X_val['item_id'].map(X_train.groupby('item_id').target.mean())\n",
    "X_val['shop_target_enc'] = X_val['shop_id'].map(X_train.groupby('shop_id').target.mean())\n",
    "X_val['category_target_enc'] = X_val['item_category_id'].map(X_train.groupby('item_category_id').target.mean())\n",
    "X_val.fillna(global_mean, inplace=True)\n",
    "X_val = downcast_dtypes(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## dropping unnessesary columns\n",
    "X_train = X_train.drop( to_drop_cols + ['shop_id', 'item_id', 'item_category_id'], axis=1)\n",
    "X_val = X_val.drop(to_drop_cols + ['shop_id', 'item_id', 'item_category_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (6186922, 33)\n",
      "Test data shape:  (238172, 33)\n"
     ]
    }
   ],
   "source": [
    "print('Train data shape: ',X_train.shape)\n",
    "print('Test data shape: ',X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for linreg is 0.474260\n",
      "Val R-squared for linreg is 0.257853\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_val.values)\n",
    "\n",
    "print('Train R-squared for linreg is %f' % r2_score(y_train, lr.predict(X_train.values)))\n",
    "print('Val R-squared for linreg is %f' % r2_score(y_val, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/arjasethan/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## mean encoding for test data\n",
    "X_test['item_target_enc'] = X_test['item_id'].map(all_data[all_data.date_block_num < last_block].groupby('item_id').target.mean())\n",
    "X_test['shop_target_enc'] = X_test['shop_id'].map(all_data[all_data.date_block_num < last_block].groupby('shop_id').target.mean())\n",
    "X_test['category_target_enc'] = X_test['item_category_id'].map(all_data[all_data.date_block_num < last_block].groupby('item_category_id').target.mean())\n",
    "X_test.fillna(global_mean, inplace=True)\n",
    "X_test = downcast_dtypes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lb score : 1.06419\n",
    "# lr_with_encoding = X_test.copy()\n",
    "# lr_with_encoding['item_cnt_month'] = lr.predict(X_test.drop( to_drop_cols + ['shop_id', 'item_id', 'item_category_id'], axis=1).values)\n",
    "# prepare_submit(lr_with_encoding, path='data/lr_with_encoding1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for LightGBM is 0.516967\n",
      "Val R-squared for LightGBM is 0.210647\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':4, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.015, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 200)\n",
    "pred_lgb = model.predict(X_val)\n",
    "\n",
    "print('Train R-squared for LightGBM is %f' % r2_score(y_train, model.predict(X_train)))\n",
    "print('Val R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras, keras.layers as L, keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net(input_shape, hidden_layers, nodes_per_layer, activation = 'relu'):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(L.InputLayer(input_shape=(input_shape,)))\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(L.Dense(nodes_per_layer, activation= activation, kernel_regularizer= keras.regularizers.l2(0.01)))\n",
    "    model.add(L.Dense(1, activation='relu'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               3400      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,501\n",
      "Trainable params: 3,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn1hidden_layer = neural_net(X_train.shape[1], hidden_layers=1, nodes_per_layer=100)\n",
    "nn1hidden_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6186922 samples, validate on 238172 samples\n",
      "Epoch 1/5\n",
      "6186922/6186922 [==============================] - 27s 4us/step - loss: 184.7815 - val_loss: 28.6339\n",
      "Epoch 2/5\n",
      "6186922/6186922 [==============================] - 27s 4us/step - loss: 12.5787 - val_loss: 28.6339\n",
      "Epoch 3/5\n",
      "6186922/6186922 [==============================] - 27s 4us/step - loss: 12.5787 - val_loss: 28.6339\n",
      "Epoch 4/5\n",
      "6186922/6186922 [==============================] - 27s 4us/step - loss: 12.5787 - val_loss: 28.6339\n",
      "Epoch 5/5\n",
      "6186922/6186922 [==============================] - 27s 4us/step - loss: 12.5787 - val_loss: 28.6339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d6b0978>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "nn1hidden_layer.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "nn1hidden_layer.fit(x=X_train, y=y_train, batch_size=500, validation_data=(X_val, y_val), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for LightGBM is -0.008499\n",
      "Val R-squared for LightGBM is -0.003118\n"
     ]
    }
   ],
   "source": [
    "nn1hidden_pred = nn1hidden_layer.predict(X_val)\n",
    "print('Train R-squared for LightGBM is %f' % r2_score(y_train, nn1hidden_layer.predict(X_train)))\n",
    "print('Val R-squared for LightGBM is %f' % r2_score(y_val, nn1hidden_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 200)               6800      \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 368,801\n",
      "Trainable params: 368,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn3hidden_layer = neural_net(X_train.shape[1], hidden_layers=10, nodes_per_layer=200, activation='tanh')\n",
    "nn3hidden_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6186922 samples, validate on 238172 samples\n",
      "Epoch 1/5\n",
      "6186922/6186922 [==============================] - 228s 37us/step - loss: 14.0680 - val_loss: 28.9193\n",
      "Epoch 2/5\n",
      "6186922/6186922 [==============================] - 215s 35us/step - loss: 14.0090 - val_loss: 29.3319\n",
      "Epoch 3/5\n",
      "6186922/6186922 [==============================] - 212s 34us/step - loss: 12.6342 - val_loss: 28.6339\n",
      "Epoch 4/5\n",
      "6186922/6186922 [==============================] - 213s 35us/step - loss: 12.5787 - val_loss: 28.6339\n",
      "Epoch 5/5\n",
      "6186922/6186922 [==============================] - 214s 35us/step - loss: 12.5787 - val_loss: 28.6339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x174a4b518>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd1 = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "nn3hidden_layer.compile(loss='mean_squared_error', optimizer=sgd1)\n",
    "nn3hidden_layer.fit(x=X_train, y=y_train, batch_size=2000, validation_data=(X_val, y_val), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for 3 layer network is 0.009177\n",
      "Val R-squared for 3 layer network is -0.007262\n"
     ]
    }
   ],
   "source": [
    "nn3hidden_pred = np.array(nn3hidden_layer.predict(X_val))\n",
    "print('Train R-squared for 3 layer network is %f' % r2_score(y_train, nn3hidden_layer.predict(X_train)))\n",
    "print('Val R-squared for 3 layer network is %f' % r2_score(y_val, nn3hidden_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for random forest is 0.578129\n",
      "Val R-squared for random forest is 0.240380\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=150, n_jobs=-1, max_depth=5, max_features='auto')\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_val)\n",
    "\n",
    "print('Train R-squared for random forest is %f' % r2_score(y_train, rf.predict(X_train)))\n",
    "print('Val R-squared for random forest is %f' % r2_score(y_val, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155495908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(len(y_val)))\n",
    "plt.scatter( x, pred_lgb)\n",
    "plt.scatter( x, y_val)\n",
    "plt.scatter( x, pred_lr)\n",
    "plt.scatter( x, rf_pred)\n",
    "plt.legend(['lgb', 'true', 'lr', 'rf'])\n",
    "plt.ylim(0,10)\n",
    "plt.xlim(0,2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 3])\n",
    "\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]:\n",
    "    \n",
    "    print(cur_block_num)\n",
    "    \n",
    "    '''\n",
    "        1. Split `X_train` into parts\n",
    "           Remember, that corresponding dates are stored in `dates_train` \n",
    "        2. Fit linear regression \n",
    "        3. Fit LightGBM and put predictions          \n",
    "        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "           You can use `dates_train_level2` for it\n",
    "           Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "    '''      \n",
    "    \n",
    "    #  YOUR CODE GOES HERE\n",
    "    X_train_block = X_train[dates_train < cur_block_num]\n",
    "    y_train_block = y_train[dates_train < cur_block_num]\n",
    "    lr_block = LinearRegression()\n",
    "    lr_block.fit(X_train_block, y_train_block)\n",
    "    lgb_block = lgb.train(lgb_params, lgb.Dataset(X_train_block, label=y_train_block), 100)\n",
    "    rf_block = RandomForestRegressor(n_estimators=150, n_jobs=-1, max_depth=5, max_features='auto')\n",
    "    rf_block.fit(X_train_block, y_train_block)\n",
    "    X_train_level2[dates_train_level2 == cur_block_num, 0] = lr_block.predict(X_train[dates_train == cur_block_num])\n",
    "    X_train_level2[dates_train_level2 == cur_block_num, 1] = lgb_block.predict(X_train[dates_train == cur_block_num])\n",
    "    X_train_level2[dates_train_level2 == cur_block_num, 2] = rf_block.predict(X_train[dates_train == cur_block_num])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for stacking is 0.270311\n",
      "Val R-squared for stacking is 0.240380\n"
     ]
    }
   ],
   "source": [
    "stacking_lr = LinearRegression()\n",
    "stacking_lr.fit(X_train_level2, y_train_level2)\n",
    "y_val_level2 = np.zeros([pred_lr.shape[0], 3])\n",
    "y_val_level2[:, 0] = pred_lr\n",
    "y_val_level2[:, 1] = pred_lgb\n",
    "y_val_level2[:, 2] = rf_pred\n",
    "pred_stacking = stacking_lr.predict(y_val_level2)\n",
    "\n",
    "print('Train R-squared for stacking is %f' % r2_score(y_train_level2, stacking_lr.predict(X_train_level2)))\n",
    "print('Val R-squared for stacking is %f' % r2_score(y_val, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
